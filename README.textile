h1. AHRD Snakemake: HPCA benchmark

"AHRD":https://github.com/groupschoof/AHRD (Automated Assignment of Human Readable Descriptions) annotates proteins with descriptions and GO terms.
The preparations to run AHRD include making protein databases locally available, performing sequence similarity searches on said databases and creating configuration files.

Snakemake is a python based worflow utility that is used here to perform all neccesarry steps to run AHRD.
In this benchmark version of the workflow the query file is the rice proteome obtained from Ensembl Genomes.

h2. Table of contents

# "Getting started":#1-getting-started
# "Workflow visualization":#2-workflow-visualization
# "Hardware requirements":#3-hardware-requirements
## "Storage Space":#31-storage-space
## "AHRD's Memory Usage":#32-ahrd's-memory-usage
## "Runtime Examples":#33-runtime-examples
# "License":#4-license
# "Authors":#5-authors


h2. 1 Getting started

* Install the python 3 version of miniconda from here: "https://docs.conda.io/en/latest/miniconda.html":https://docs.conda.io/en/latest/miniconda.html

* Install mamba and git:
@conda install -c conda-forge -c bioconda mamba git@

* Clone the ahrd_snakemake pipeline:
@git clone --branch "hpca_benchmark" https://github.com/groupschoof/AHRD_Snakemake.git@

* Create an empty conda environment:
@conda create --name ahrd_snakemake@

* ... and use mamba to install packages in it faster than with conda:
@mamba env update --name ahrd_snakemake --file AHRD_Snakemake/workflow/environment.yaml@

* Activate the conda environment:
@conda activate ahrd_snakemake@

* Start the download of all necessary data (performed before the benchmark to keep it from skewing results)
@snakemake --use-conda --cores 16 --until download_all@

* Start the workflow:
@snakemake --use-conda --cores 64 --stats stats.log@

* Once the workflow is finished you will find the total runtime in the stats.log file

h2. 2 Workflow visualization

@snakemake --rulegraph | dot -Tsvg > rulegraph.svg@
!./rulegraph.svg!

h2. 3 Hardware requirements

h3. 3.1 Storage Space

| resources | Reference GO annotations | 13 GB |
| | SwissProt | 0.09 GB |
| | Uniref90 | 30 GB |
| | Query Fasta Files | each 0.02-0.13 GB |
| results | SwissProt DiamondDB | 2.74 GB |
| | Uniref90 DiamondDB | 60 GB |
| | SwissProt Search Results | each 0.2-1.5 GB |
| | Uniref 90 Search Results | each 0.6-5 GB |
| workflow | snakemake folder | 1.6 GB |
| Overall | | ca. 130 GB|

The resources are automatically downloaded (except the query fasta files).
So this also indicates the bandwidth / data usage requirements.

h3. 3.2 AHRD's Memory Usage

Formula:
11000+fastaSizeInMB*540 = projectedMemUsageInMB

Example 1 (<i>Oryza sativa</i>):
11000 + 23 MB * 540 = 23420 MB = 23GB

Example 2 (<i>Hordeum vulgare</i>):
11000 + 131 MB * 540 = 81740 MB = 82GB

h3. 3.3 Runtime Examples

| Download | Reference GO annotations | | 4min |
| | Uniref90 | | 3h |
| Diamond | Create Uniref90 | 48 Cores | 21min |
| | Search Rize in Uniref90 | 32 Cores | 1.5h |
| | Search Barley in Uniref90 | 32 Cores | 4h |
| AHRD | Annotate Rize | 8 Cores | 3.3h |
| | Annotate Barley | 8 Cores | 3.5h |

Download times depend on your location relative to the uniprot servers, how busy the servers are and of course your connection.

Diamond scales very well. Give it more cores and memory and it will put them to good use.

AHRD's bootleneck is parsing the data. The actual annotation step is quick by comparison. Nonetheless,  it's parallelized but doesn't scale very well.

h2. 4 License

See attached file LICENSE.txt for details.

h2. 5 Authors

Dr. Florian Boecker and Prof. Dr. Heiko Schoof

INRES Crop Bioinformatics
University of Bonn
Katzenburgweg 2
53115 Bonn
Germany
